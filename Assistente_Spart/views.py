from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated
from core.utils import get_licenca_db_config
from core.middleware import get_licenca_slug
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from openai import OpenAI
import base64, tempfile, logging, time
from .agenteReact import agenteReact, AGENT_TOOLS
from .tools.describer import gerar_descricao_tools
from .tools.qa_tools import faiss_context_qa
from core.utils import configurar_logger_colorido

# === Logger colorido ===
configurar_logger_colorido()
logger = logging.getLogger(__name__)

# === Descri√ß√£o din√¢mica das tools ===
descricao_tools = gerar_descricao_tools(AGENT_TOOLS)


class SpartView(APIView):
    permission_classes = [IsAuthenticated]

    def post(self, request, slug=None):
        inicio_total = time.time()
        client = OpenAI()

        # ======== CONTEXTO EMPRESA/FILIAL/BANCO ========
        slug = get_licenca_slug()
        banco = get_licenca_db_config(request)
        empresa_id = request.META.get("HTTP_X_EMPRESA", 1)
        filial_id = request.META.get("HTTP_X_FILIAL", 1)

        config = RunnableConfig(configurable={
            "thread_id": str(request.user.usua_codi),
            "empresa_id": str(empresa_id),
            "filial_id": str(filial_id),
            "banco": banco,
            "slug": slug,
        })

        # ======== ENTRADA DO USU√ÅRIO ========
        mensagem_usuario = None
        if "mensagem" in request.data:
            mensagem_usuario = request.data.get("mensagem")
            logger.info("üìù Entrada via texto")

        elif "audio" in request.FILES:
            inicio_whisper = time.time()
            logger.info("üé§ Iniciando transcri√ß√£o de √°udio...")
            audio_file = request.FILES["audio"]
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                for chunk in audio_file.chunks():
                    tmp.write(chunk)
                tmp_path = tmp.name

            with open(tmp_path, "rb") as f:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=f,
                    language="pt"
                )
            mensagem_usuario = transcript.text
            logger.info(f"‚úÖ Transcri√ß√£o conclu√≠da em {round(time.time() - inicio_whisper, 2)}s: {mensagem_usuario[:50]}...")

        if not isinstance(mensagem_usuario, str):
            return Response({"erro": "mensagem deve ser texto ou √°udio"}, status=400)

        # ======== CONTEXTO FAISS OPCIONAL ========
        inicio_faiss = time.time()
        try:
            logger.info("üîç Buscando contexto no FAISS...")
            contexto_faiss = faiss_context_qa.invoke({"pergunta": mensagem_usuario})
            logger.info(f"‚úÖ FAISS conclu√≠do em {round(time.time() - inicio_faiss, 2)}s")
        except Exception as e:
            logger.warning(f"[FAISS] Erro ao buscar contexto em {round(time.time() - inicio_faiss, 2)}s: {e}")
            contexto_faiss = None

        # ======== PROMPT UNIFICADO ========
        if contexto_faiss:
            prompt = f"""
{descricao_tools}

üìé Contexto de apoio (FAISS):
{contexto_faiss}

üìç Contexto atual:
- Empresa: {empresa_id}
- Filial: {filial_id}
- Banco: {banco}
- Slug: {slug}

üí¨ Pergunta: {mensagem_usuario}
"""
        else:
            prompt = f"""
{descricao_tools}

üìç Contexto atual:
- Empresa: {empresa_id}
- Filial: {filial_id}
- Banco: {banco}
- Slug: {slug}

üí¨ Pergunta: {mensagem_usuario}
"""

        logger.debug(f"[PROMPT_PREVIEW]\n{prompt[:600]}...\n---")

        # ======== EXECU√á√ÉO DO AGENTE (SIMPLES) ========
        resposta_texto = ""
        inicio_agente = time.time()
        try:
            logger.info("ü§ñ Executando agente ReAct...")
            resultado = agenteReact.invoke(
                {"messages": [HumanMessage(content=prompt)]},
                config
            )
            logger.info(f"‚úÖ Agente conclu√≠do em {round(time.time() - inicio_agente, 2)}s")
            
            # Extrai a resposta final
            mensagens = resultado.get("messages", [])
            
            # Pega a √∫ltima mensagem do assistente
            for msg in reversed(mensagens):
                if hasattr(msg, '__class__') and msg.__class__.__name__ == 'AIMessage':
                    conteudo = msg.content
                    if isinstance(conteudo, list):
                        conteudo = " ".join(
                            str(b.get("text", "")) for b in conteudo
                            if isinstance(b, dict) and b.get("type") == "text"
                        )
                    if conteudo:
                        resposta_texto = str(conteudo).strip()
                        break
            
            logger.info(f"‚úÖ Resposta gerada: {resposta_texto[:100]}...")
            
        except Exception as e:
            logger.exception("[SpartView] Erro ao executar agente")
            resposta_texto = f"‚ùå Erro ao processar sua solicita√ß√£o: {str(e)}"

        # ======== FALLBACK ========
        if not resposta_texto:
            resposta_texto = "‚ö†Ô∏è O agente n√£o retornou uma resposta. Tente novamente."

        # ======== GERAR √ÅUDIO TTS ========
        audio_base64 = ""
        inicio_tts = time.time()
        try:
            logger.info("üîä Gerando √°udio TTS...")
            tts_response = client.audio.speech.create(
                model="gpt-4o-mini-tts",
                voice="alloy",
                input=resposta_texto
            )
            audio_base64 = base64.b64encode(tts_response.read()).decode("utf-8")
            logger.info(f"‚úÖ TTS conclu√≠do em {round(time.time() - inicio_tts, 2)}s")
        except Exception as e:
            logger.warning(f"[SpartView] Falha ao gerar √°udio TTS em {round(time.time() - inicio_tts, 2)}s: {e}")

        fim_total = time.time()
        logger.info(f"‚úÖ Tempo total: {round(fim_total - inicio_total, 2)}s")

        return Response({
            "resposta": resposta_texto,
            "resposta_audio": audio_base64
        })